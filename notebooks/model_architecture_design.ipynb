{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2c9c7f5",
   "metadata": {},
   "source": [
    "# Call Centre Ticket Classification - ML Model Architecture Design\n",
    "\n",
    "## üìã Project Overview\n",
    "\n",
    "**Objective**: Design and implement a text classification system to automatically categorize call centre tickets into 6 predefined categories with ‚â•85% accuracy.\n",
    "\n",
    "**Categories**: \n",
    "- **BILLING**: Account, payment, billing issues\n",
    "- **TECHNICAL**: Network, connectivity, service problems  \n",
    "- **SALES**: New services, upgrades, product inquiries\n",
    "- **COMPLAINTS**: Service complaints, escalations\n",
    "- **NETWORK**: Infrastructure and network-related issues\n",
    "- **ACCOUNT**: Account management, profile changes\n",
    "\n",
    "**Success Criteria**:\n",
    "- Classification accuracy ‚â•85% on test data\n",
    "- Processing speed <2 seconds per ticket\n",
    "- Production-ready FastAPI deployment\n",
    "- 80% test coverage\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Experiment Documentation\n",
    "\n",
    "**Hypothesis**: Transformer-based models (BERT/DistilBERT) will outperform traditional ML approaches for this multi-class text classification task due to better contextual understanding of telecoms terminology.\n",
    "\n",
    "**Baseline Models to Test**:\n",
    "1. **Traditional**: Logistic Regression + TF-IDF\n",
    "2. **Ensemble**: Random Forest + TF-IDF\n",
    "3. **Transformer**: DistilBERT fine-tuned\n",
    "4. **Hybrid**: Ensemble of traditional + transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc46f38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd  # type: ignore\n",
    "import matplotlib.pyplot as plt  # type: ignore\n",
    "import seaborn as sns  # type: ignore\n",
    "from sklearn.model_selection import train_test_split  # type: ignore\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # type: ignore\n",
    "from sklearn.linear_model import LogisticRegression  # type: ignore\n",
    "from sklearn.metrics import classification_report, accuracy_score  # type: ignore\n",
    "from sklearn.pipeline import Pipeline  # type: ignore\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üìö Libraries imported successfully!\")\n",
    "print(\"üîß Environment setup complete\")\n",
    "\n",
    "# Configuration\n",
    "RANDOM_STATE = 42\n",
    "TARGET_ACCURACY = 0.85\n",
    "MAX_PROCESSING_TIME = 2.0  # seconds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f73546",
   "metadata": {},
   "source": [
    "## üìä Data Analysis & Preparation\n",
    "\n",
    "First, let's generate our mock dataset and perform initial exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a382e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from data.mock_data_generator import TelecomsTicketGenerator\n",
    "\n",
    "# Initialize generator and create dataset\n",
    "generator = TelecomsTicketGenerator(seed=RANDOM_STATE)\n",
    "dataset = generator.generate_dataset()\n",
    "\n",
    "print(\"üìà Dataset Overview:\")\n",
    "print(f\"   Total samples: {len(dataset):,}\")\n",
    "print(f\"   Features: {list(dataset.columns)}\")\n",
    "print(\"   Target distribution:\")\n",
    "print(dataset['category'].value_counts().sort_index())\n",
    "\n",
    "# Display sample tickets\n",
    "print(\"\\nüìù Sample Tickets:\")\n",
    "for category in dataset['category'].unique()[:3]:\n",
    "    sample = dataset[dataset['category'] == category]['ticket_text'].iloc[0]\n",
    "    print(f\"\\n{category}: {sample[:100]}...\")\n",
    "\n",
    "# Basic statistics\n",
    "dataset.info()\n",
    "dataset.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0455b8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Category distribution\n",
    "dataset['category'].value_counts().plot(kind='bar', ax=axes[0,0], color='skyblue')\n",
    "axes[0,0].set_title('Ticket Category Distribution')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Text length distribution\n",
    "dataset['text_length'] = dataset['ticket_text'].str.len()\n",
    "dataset['text_length'].hist(bins=50, ax=axes[0,1], color='lightgreen')\n",
    "axes[0,1].set_title('Ticket Text Length Distribution')\n",
    "axes[0,1].set_xlabel('Characters')\n",
    "\n",
    "# Priority distribution by category\n",
    "priority_cat = pd.crosstab(dataset['category'], dataset['priority'])\n",
    "priority_cat.plot(kind='bar', stacked=True, ax=axes[1,0])\n",
    "axes[1,0].set_title('Priority Distribution by Category')\n",
    "axes[1,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Customer type distribution\n",
    "dataset['customer_type'].value_counts().plot(kind='pie', ax=axes[1,1], autopct='%1.1f%%')\n",
    "axes[1,1].set_title('Customer Type Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Text statistics by category\n",
    "print(\"üìä Text Length Statistics by Category:\")\n",
    "print(dataset.groupby('category')['text_length'].agg(['mean', 'median', 'std']).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22798dc",
   "metadata": {},
   "source": [
    "## üîß Feature Engineering & Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a305c2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess ticket text.\"\"\"\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits (keep some punctuation for context)\n",
    "    text = re.sub(r'[^\\w\\s\\.\\,\\!\\?]', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "dataset['processed_text'] = dataset['ticket_text'].apply(preprocess_text)\n",
    "\n",
    "# Feature Engineering: Extract additional features\n",
    "def extract_features(df):\n",
    "    \"\"\"Extract additional features from ticket data.\"\"\"\n",
    "    features = df.copy()\n",
    "    \n",
    "    # Text features\n",
    "    features['word_count'] = features['processed_text'].str.split().str.len()\n",
    "    features['char_count'] = features['processed_text'].str.len()\n",
    "    features['avg_word_length'] = features['char_count'] / features['word_count']\n",
    "    \n",
    "    # Telecoms-specific keywords\n",
    "    billing_keywords = ['bill', 'payment', 'charge', 'cost', 'money', 'rand', 'debit', 'account']\n",
    "    technical_keywords = ['internet', 'connection', 'speed', 'wifi', 'router', 'signal', 'slow']\n",
    "    \n",
    "    features['has_billing_keywords'] = features['processed_text'].str.contains('|'.join(billing_keywords))\n",
    "    features['has_technical_keywords'] = features['processed_text'].str.contains('|'.join(technical_keywords))\n",
    "    \n",
    "    # Urgency indicators\n",
    "    urgency_keywords = ['urgent', 'asap', 'immediately', 'emergency', 'critical']\n",
    "    features['has_urgency'] = features['processed_text'].str.contains('|'.join(urgency_keywords))\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Apply feature engineering\n",
    "dataset_enhanced = extract_features(dataset)\n",
    "\n",
    "print(\"üîß Feature Engineering Complete:\")\n",
    "print(f\"   New features added: {set(dataset_enhanced.columns) - set(dataset.columns)}\")\n",
    "print(f\"   Total features: {len(dataset_enhanced.columns)}\")\n",
    "\n",
    "# Show sample of enhanced features\n",
    "enhanced_sample = dataset_enhanced[['category', 'word_count', 'char_count', 'has_billing_keywords', 'has_technical_keywords']].head()\n",
    "print(\"\\nüìä Enhanced Feature Sample:\")\n",
    "print(enhanced_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374a06be",
   "metadata": {},
   "source": [
    "## ü§ñ Model Architecture & Training Pipeline\n",
    "\n",
    "Now let's design and implement multiple model architectures to find the best performing approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4b3e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = dataset_enhanced['processed_text']\n",
    "y = dataset_enhanced['category']\n",
    "\n",
    "# Stratified split to maintain class balance\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"üìä Data Split Summary:\")\n",
    "print(f\"   Training: {len(X_train):,} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"   Validation: {len(X_val):,} samples ({len(X_val)/len(X)*100:.1f}%)\")  \n",
    "print(f\"   Test: {len(X_test):,} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Verify class balance is maintained\n",
    "print(\"\\nüéØ Class Distribution Verification:\")\n",
    "print(\"Training set:\", y_train.value_counts().sort_index().tolist())\n",
    "print(\"Validation set:\", y_val.value_counts().sort_index().tolist())\n",
    "print(\"Test set:\", y_test.value_counts().sort_index().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917901be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create baseline pipeline\n",
    "tfidf_lr_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=10000,\n",
    "        ngram_range=(1, 2),\n",
    "        stop_words='english',\n",
    "        min_df=2,\n",
    "        max_df=0.95\n",
    "    )),\n",
    "    ('classifier', LogisticRegression(\n",
    "        random_state=RANDOM_STATE,\n",
    "        max_iter=1000,\n",
    "        class_weight='balanced'\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train and evaluate\n",
    "print(\"üöÄ Training Baseline Model (Logistic Regression + TF-IDF)...\")\n",
    "start_time = time.time()\n",
    "tfidf_lr_pipeline.fit(X_train, y_train)\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "# Predictions and evaluation\n",
    "start_time = time.time()\n",
    "y_pred_lr = tfidf_lr_pipeline.predict(X_val)\n",
    "y_pred_proba_lr = tfidf_lr_pipeline.predict_proba(X_val)\n",
    "inference_time = (time.time() - start_time) / len(X_val)\n",
    "\n",
    "# Performance metrics\n",
    "lr_accuracy = accuracy_score(y_val, y_pred_lr)\n",
    "\n",
    "print(\"‚úÖ Baseline Results:\")\n",
    "print(f\"   Accuracy: {lr_accuracy:.4f}\")\n",
    "print(f\"   Training time: {training_time:.2f} seconds\")\n",
    "print(f\"   Avg inference time: {inference_time*1000:.2f} ms per sample\")\n",
    "print(f\"   Target achieved: {'‚úÖ YES' if lr_accuracy >= TARGET_ACCURACY else '‚ùå NO'}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nüìä Detailed Classification Report:\")\n",
    "print(classification_report(y_val, y_pred_lr))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
